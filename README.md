# Word2Vec Project

## Overview
This project utilizes the Word2Vec model to generate and analyze word embeddings using PyTorch. The focus is on understanding how different configurations impact the quality of embeddings and applying these embeddings in an attention-based classifier. This repository includes Python notebooks for training, analysis, and classification tasks.

## Project Structure

### WORD2VEC
- Attention Classifier Pytorch.ipynb # Notebook for classification using attention mechanism
- Word2Vec Pytorch.ipynb # Notebook for training the Word2Vec model
- Word2vec vector analysis-1.ipynb # Notebook for analyzing the vector embeddings


## Features
- **Word Embedding Training**: Utilize PyTorch to train Word2Vec models, exploring both Skip-gram and CBOW architectures.
- **Embedding Analysis**: Perform qualitative and quantitative analysis of word embeddings to assess their effectiveness.
- **Attention-based Classification**: Apply trained embeddings within an attention-based classifier to demonstrate practical NLP applications.

## Getting Started

### Prerequisites
You will need Python 3.6 or later, with PyTorch and Jupyter installed. Install necessary libraries using:

### Running the Notebooks
To use the notebooks, start the Jupyter Notebook server:

```bash
jupyter notebook
```

Open any of the notebooks in the WORD2VEC directory to start executing the code.

### Training the Model
Open the Word2Vec Pytorch.ipynb notebook to train your Word2Vec model using PyTorch.

### Evaluating and Analyzing Embeddings
Review the Word2vec vector analysis-1.ipynb notebook for steps on analyzing the embeddings.

### Using the Attention Classifier
Explore the Attention Classifier Pytorch.ipynb to see how the embeddings can be used in an advanced classifier model.